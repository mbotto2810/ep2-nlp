{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CsCJSgEc_ZKP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch import nn, optim\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "from scipy.stats import pearsonr\n",
        "\n",
        "!pip install transformers torch\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "SAMPLES = 10000\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 3"
      ],
      "metadata": {
        "id": "3M6otZ-D7zH3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpYMIv8R_ZKV"
      },
      "source": [
        "# Data pre processing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "EMCSFSxRBETz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = '/content/drive/MyDrive/B2W-Reviews01.csv'"
      ],
      "metadata": {
        "id": "TFqx2TvuBMwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0sJOuYt0_ZKY"
      },
      "outputs": [],
      "source": [
        "df_raw = pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I0U-Bp1N_ZKa"
      },
      "outputs": [],
      "source": [
        "def calculate_vowel_density(text):\n",
        "    text = re.sub(r'[^A-Za-zÇçÃãÁáÉéÍíÓóÚúÀàÊêÔô]', '', text)  # remove non alphabetical chars\n",
        "    total_letters = len(text)\n",
        "    total_vowels = len(re.findall(r'[AEIOUaeiouÁÉÍÓÚáéíóúÀàÃãÊêÔô]', text))\n",
        "    return total_vowels / total_letters if total_letters > 0 else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iOJRNYhu_ZKa"
      },
      "outputs": [],
      "source": [
        "def preprocessing(df):\n",
        "    cols = ['review_text']\n",
        "    df = df[cols]\n",
        "    df.rename(columns={'review_text': 'text'}, inplace=True)\n",
        "    df = df.dropna()\n",
        "    return df\n",
        "\n",
        "df = preprocessing(df_raw)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JuB27R4J_ZKa"
      },
      "source": [
        "# Regression on vowel quantities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuBA8-Ls_ZKa"
      },
      "outputs": [],
      "source": [
        "df['density'] = df['text'].apply(calculate_vowel_density)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaHDw2rZ_ZKb"
      },
      "outputs": [],
      "source": [
        "# 10 000  samples to speed up\n",
        "df = df.sample(n=SAMPLES)\n",
        "print(len(df))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yk6Lsiym_ZKb"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_df, val_df  = train_test_split(train_df, test_size=0.25, random_state=42)\n",
        "\n",
        "print(len(train_df), 'train examples')\n",
        "print(len(val_df), 'validation examples')\n",
        "print(len(test_df), 'test examples')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1YAsCeT6_ZKb"
      },
      "outputs": [],
      "source": [
        "len(train_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TlMsH8fw_ZKb"
      },
      "source": [
        "## BERTimbau fine tunning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GF5AG0AL_ZKc"
      },
      "outputs": [],
      "source": [
        "# Tokenizer and BERTimbau\n",
        "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "model = BertModel.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, density):\n",
        "        self.text    = text\n",
        "        self.density = density\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text[idx]\n",
        "        density = self.density[idx]\n",
        "        inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
        "        return inputs, torch.tensor(density, dtype=torch.float)\n",
        "\n",
        "# TextoDataset\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "train_dataset = TextDataset(train_df['text'], train_df['density'])\n",
        "validate_dataset = TextDataset(val_df['text'], val_df['density'])\n",
        "test_dataset = TextDataset(test_df['text'], test_df['density'])\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "validate_loader = DataLoader(validate_dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKZLJNiV_ZKc"
      },
      "outputs": [],
      "source": [
        "class BertForRegression(nn.Module):\n",
        "    def __init__(self, bert_model):\n",
        "        super(BertForRegression, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        self.regressor = nn.Linear(768, 1)  # 768 is the dimensions of the characterist vector of BERT\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        return self.regressor(pooled_output)\n",
        "\n",
        "model = BertForRegression(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o6Tg1Mtr_ZKc"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")\n",
        "model.to(device)\n",
        "\n",
        "# Loss criterion and optimizer\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxaCI98M_ZKc"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, validate_loader, epochs=3):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Train loop with tqdm\n",
        "        train_loop = tqdm(train_loader, leave=True)\n",
        "        for inputs, labels in train_loop:\n",
        "            input_ids = inputs['input_ids'].to(device)\n",
        "            attention_mask = inputs['attention_mask'].to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            input_ids = input_ids.view(-1, input_ids.size(-1))\n",
        "            attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            outputs = outputs.view(-1, 1)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            # Update tqdm description with current loss\n",
        "            train_loop.set_description(f'Epoch {epoch+1}')\n",
        "            train_loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        print('\\n Validating...')\n",
        "\n",
        "        # Validation loop with tqdm\n",
        "        validate_loop = tqdm(validate_loader, leave=True)\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in validate_loop:\n",
        "                input_ids = inputs['input_ids'].to(device)\n",
        "                attention_mask = inputs['attention_mask'].to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                input_ids = input_ids.view(-1, input_ids.size(-1))\n",
        "                attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n",
        "\n",
        "                outputs = model(input_ids, attention_mask)\n",
        "                outputs = outputs.view(-1, 1)\n",
        "\n",
        "                # Colocar ideia aqui\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "\n",
        "                validate_loop.set_postfix(val_loss=loss.item())\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Training Loss: {running_loss/len(train_loader)}, Validation Loss: {val_loss/len(validate_loader)}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gSZR-tFX_ZKc"
      },
      "outputs": [],
      "source": [
        "train_model(model, criterion, optimizer, train_loader, validate_loader, epochs=EPOCHS)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the entire model\n",
        "torch.save(model, 'reg_model.pth')"
      ],
      "metadata": {
        "id": "1qVR4fZSG2HE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "27n_0GQOf6Kf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERkI_HZK_ZKd"
      },
      "outputs": [],
      "source": [
        "def rmse(y_true, y_pred):\n",
        "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "\n",
        "def mae(y_true, y_pred):\n",
        "    return mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "def mape(y_true, y_pred):\n",
        "    y_true += 1e-5 # Numeric problems\n",
        "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
        "\n",
        "def pearson_correlation(y_true, y_pred):\n",
        "    corr, _ = pearsonr(y_true, y_pred)\n",
        "    return corr"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def eval_test(model, test_loader):\n",
        "    y_pred_model = []\n",
        "    y_true       = []\n",
        "\n",
        "    # Evaluating the model of density of vowels per sentence on the test set\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            input_ids = inputs['input_ids'].to(device)\n",
        "            attention_mask = inputs['attention_mask'].to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            input_ids = input_ids.view(-1, input_ids.size(-1))\n",
        "            attention_mask = attention_mask.view(-1, attention_mask.size(-1))\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            outputs = outputs.view(-1, 1)\n",
        "\n",
        "            y_pred_model.extend(outputs.view(-1).cpu().numpy())\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Convert predictions and true values to numpy arrays\n",
        "    y_pred_model = np.array(y_pred_model)\n",
        "    y_true = np.array(y_true)\n",
        "    return y_true, y_pred_model"
      ],
      "metadata": {
        "id": "lsEdTTxN4JJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zOueK47_ZKd"
      },
      "outputs": [],
      "source": [
        "# Evaluating the model of density of vowels per sentence on the test set\n",
        "y_true_sentence, y_pred_model_sentence = eval_test(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model of density of vowels on the global density of the test\n",
        "# Use the train dataset to prevent leakage\n",
        "global_density = list(train_df.text.values)\n",
        "separator = ' '\n",
        "global_density = calculate_vowel_density(separator.join(global_density))\n",
        "\n",
        "test_df2 = test_df.copy()\n",
        "test_df2 = test_df2.reset_index(drop=True)\n",
        "test_df2['density'] = global_density\n",
        "\n",
        "test_dataset2 = TextDataset(test_df2['text'], test_df2['density'])\n",
        "test_loader2 = DataLoader(test_dataset2, batch_size=BATCH_SIZE)\n",
        "\n",
        "y_true_global, y_pred_model_global = eval_test(model, test_loader2)"
      ],
      "metadata": {
        "id": "bXEW-dHe4YSh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model of density of vowels on the first word density\n",
        "def get_first_word_density(text):\n",
        "    list_word = text.split(' ')\n",
        "    word = list_word[0]\n",
        "    return calculate_vowel_density(word)\n",
        "\n",
        "first_density = list(test_df['text'].apply(lambda x : get_first_word_density(x)).values)\n",
        "\n",
        "test_df3 = test_df.copy()\n",
        "test_df3 = test_df3.reset_index(drop=True)\n",
        "test_df3['density'] = first_density\n",
        "\n",
        "test_dataset3 = TextDataset(test_df3['text'], test_df3['density'])\n",
        "test_loader3 = DataLoader(test_dataset3, batch_size=BATCH_SIZE)\n",
        "\n",
        "y_true_first, y_pred_model_first = eval_test(model, test_loader3)"
      ],
      "metadata": {
        "id": "42JVmWtr4YVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluating the model of density of vowels on the last word density\n",
        "def get_last_word_density(text):\n",
        "    list_word = text.split(' ')\n",
        "    word = list_word[len(list_word)-1]\n",
        "    return calculate_vowel_density(word)\n",
        "\n",
        "last_density = list(test_df['text'].apply(lambda x : get_last_word_density(x)).values)\n",
        "\n",
        "test_df4 = test_df.copy()\n",
        "test_df4 = test_df4.reset_index(drop=True)\n",
        "test_df4['density'] = last_density\n",
        "\n",
        "test_dataset4 = TextDataset(test_df4['text'], test_df4['density'])\n",
        "test_loader4 = DataLoader(test_dataset4, batch_size=BATCH_SIZE)\n",
        "\n",
        "y_true_last, y_pred_model_last = eval_test(model, test_loader4)\n"
      ],
      "metadata": {
        "id": "jAUna0gQ4nZT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_metrics(y_true, y_pred_model):\n",
        "    return {\n",
        "        \"RMSE\": rmse(y_true, y_pred_model),\n",
        "        \"MAE\": mae(y_true, y_pred_model),\n",
        "        \"MAPE\": mape(y_true, y_pred_model),\n",
        "        \"R2\": r2_score(y_true, y_pred_model),\n",
        "        \"Pearson\": pearson_correlation(y_true, y_pred_model)\n",
        "    }"
      ],
      "metadata": {
        "id": "Ka-L3nqS2uDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_dict_first = get_metrics(y_true_first, y_pred_model_first)\n",
        "metrics_dict_last = get_metrics(y_true_last, y_pred_model_last)\n",
        "metrics_dict_sentence = get_metrics(y_true_sentence, y_pred_model_sentence)\n",
        "metrics_dict_global = get_metrics(y_true_global, y_pred_model_global)\n",
        "\n",
        "metrics_df = pd.DataFrame([metrics_dict_first,\n",
        "                           metrics_dict_last,\n",
        "                           metrics_dict_global,\n",
        "                           metrics_dict_sentence], index=\n",
        "                            ['Using only the first word',\n",
        "                             'Using only the last word',\n",
        "                             'Using the global density',\n",
        "                             'Using the density per sentence'])\n",
        "metrics_df"
      ],
      "metadata": {
        "id": "tolFkmlw-fFZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classification problem"
      ],
      "metadata": {
        "id": "sxd-6vHzCFu2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ljCKsfKy_ZKd"
      },
      "outputs": [],
      "source": [
        "def classify_vowel_density(density):\n",
        "    if density < 1/3:\n",
        "        return 0  # Class 1\n",
        "    elif 1/3 <= density <= 2/3:\n",
        "        return 1  # Class 2\n",
        "    else:\n",
        "        return 2  # Class 3\n",
        "\n",
        "# Applying the classification function to the 'densidades' column\n",
        "df['labels'] = df['density'].apply(classify_vowel_density)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "df['labels'].hist(log=True)\n",
        "plt.title('Distribution of labels on log scale')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kcKNr9dNAhv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xlGQJP7_ZKd"
      },
      "outputs": [],
      "source": [
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "train_df, val_df  = train_test_split(train_df, test_size=0.25, random_state=42)\n",
        "\n",
        "print(len(train_df), 'train examples')\n",
        "print(len(val_df), 'validation examples')\n",
        "print(len(test_df), 'test examples')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classe do Dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, labels):\n",
        "        self.text = text\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text[idx]\n",
        "        label = self.labels[idx]\n",
        "        inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
        "        input_ids = inputs['input_ids'].squeeze(0)  # Remove batch dimension\n",
        "        attention_mask = inputs['attention_mask'].squeeze(0)  # Remove batch dimension\n",
        "        return input_ids, attention_mask, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# Preparando DataFrames (assumindo que você já os tem divididos em treino, validação e teste)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "# Substitua 'densidades' por uma coluna apropriada de etiquetas de classe\n",
        "train_dataset = TextDataset(train_df['text'], train_df['labels'])\n",
        "validate_dataset = TextDataset(val_df['text'], val_df['labels'])\n",
        "test_dataset = TextDataset(test_df['text'], test_df['labels'])\n",
        "\n",
        "# Criar DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "validate_loader = DataLoader(validate_dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n"
      ],
      "metadata": {
        "id": "ofZAcJolCM4R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining and training the model"
      ],
      "metadata": {
        "id": "j-hVPDq1DUcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Unbalanced case"
      ],
      "metadata": {
        "id": "SpaJSEfhEhKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BertForClassification(nn.Module):\n",
        "    def __init__(self, bert_model, num_classes):\n",
        "        super(BertForClassification, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        # Assuming 768-dimensional feature vectors from BERT, with num_classes outputs\n",
        "        self.classifier = nn.Linear(768, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        return self.classifier(pooled_output)\n",
        "\n",
        "# Example usage\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')  # Use a pre-trained BERT model\n",
        "model = BertForClassification(bert_model, num_classes=3)  # Specify the number of classes\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Loss function and optimizer for classification (using CrossEntropyLoss for multi-class)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)"
      ],
      "metadata": {
        "id": "chk146AnDV7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, validate_loader, num_classes, epochs=3):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Train loop with tqdm\n",
        "        train_loop = tqdm(train_loader, leave=True)\n",
        "        for input_ids, attention_mask, labels in train_loop:\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "\n",
        "            # For classification, outputs are likely to be logits, need to reshape for CrossEntropyLoss\n",
        "            outputs = outputs.view(-1, num_classes)  # num_classes is the number of classes in your classification task\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Update tqdm description with current loss\n",
        "        train_loop.set_description(f'Epoch {epoch+1}')\n",
        "        train_loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        print('\\n Validating...')\n",
        "\n",
        "        # Validation loop with tqdm\n",
        "        validate_loop = tqdm(validate_loader, leave=True)\n",
        "        with torch.no_grad():\n",
        "            for input_ids, attention_mask, labels in validate_loop:\n",
        "                input_ids = input_ids.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(input_ids, attention_mask)\n",
        "                outputs = outputs.view(-1, num_classes)  # Adjusted to match the number of classes\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                validate_loop.set_postfix(val_loss=loss.item())\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Training Loss: {running_loss/len(train_loader)}, Validation Loss: {val_loss/len(validate_loader)}\")"
      ],
      "metadata": {
        "id": "CWFv4BMdDXTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, criterion, optimizer, train_loader, validate_loader, epochs=EPOCHS, num_classes=3)"
      ],
      "metadata": {
        "id": "T2iBC16sEQDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the entire model\n",
        "torch.save(model, 'classification_model_unbalanced.pth')"
      ],
      "metadata": {
        "id": "P0zCjlmGF27S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "lZKFsHUPfMyN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_mask, labels in test_loader:\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return np.array(all_preds), np.array(all_labels)\n",
        "\n",
        "# Get predictions and true labels\n",
        "preds, labels = evaluate_model(model, test_loader, device)\n",
        "\n",
        "# Total accuracy\n",
        "total_accuracy = accuracy_score(labels, preds)\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(labels, preds)\n",
        "per_class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "sensitivity = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
        "specificity = np.diag(conf_matrix) / np.sum(conf_matrix, axis=0)\n",
        "\n",
        "# Output the metrics\n",
        "print(\"Total Accuracy:\", total_accuracy)\n",
        "print(\"Per Class Accuracy:\", per_class_accuracy)\n",
        "print(\"Sensitivity (Recall) per Class:\", sensitivity)\n",
        "print(\"Specificity per Class:\", specificity)\n"
      ],
      "metadata": {
        "id": "G4o1u283KjLz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Balanced case"
      ],
      "metadata": {
        "id": "v7KHRw3jEjXd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Make the dataset baalnced\n",
        "df = preprocessing(df_raw)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Uy9ExBKhEkTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)"
      ],
      "metadata": {
        "id": "0mS6BD_U83tq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['densidades'] = df['texto'].apply(calculate_vowel_density)\n",
        "df['labels'] = df['densidades'].apply(classify_vowel_density)"
      ],
      "metadata": {
        "id": "b7yu9lZ58_QX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "min_label = 0\n",
        "for i in list(np.unique(df.labels.values)):\n",
        "    if len(df[df['labels'] == i]) < len(df[df['labels']==min_label]):\n",
        "        min_label = i\n",
        "\n",
        "# resample of the classes\n",
        "samples = []\n",
        "n_samples = len(df[df['labels']==min_label])\n",
        "for i in list(np.unique(df.labels.values)):\n",
        "    df_class = df[ df['labels'] == i ]\n",
        "    samples.append( df_class.sample(n=n_samples) )\n",
        "\n",
        "df_balanced = pd.concat(samples)\n",
        "df_balanced['labels'].hist()"
      ],
      "metadata": {
        "id": "L5j8Px9D9Np2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df, test_df = train_test_split(df_balanced, test_size=0.2, random_state=42)\n",
        "train_df, val_df  = train_test_split(train_df, test_size=0.25, random_state=42)\n",
        "\n",
        "print(len(train_df), 'train examples')\n",
        "print(len(val_df), 'validation examples')\n",
        "print(len(test_df), 'test examples')"
      ],
      "metadata": {
        "id": "usl8AvK8AKw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "model = BertModel.from_pretrained('neuralmind/bert-base-portuguese-cased')\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, text, labels):\n",
        "        self.text = text\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.text)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = self.text[idx]\n",
        "        label = self.labels[idx]\n",
        "        inputs = tokenizer(text, return_tensors='pt', max_length=512, truncation=True, padding='max_length')\n",
        "        input_ids = inputs['input_ids'].squeeze(0)  # Remove batch dimension\n",
        "        attention_mask = inputs['attention_mask'].squeeze(0)  # Remove batch dimension\n",
        "        return input_ids, attention_mask, torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "# Preparando DataFrames (assumindo que você já os tem divididos em treino, validação e teste)\n",
        "train_df = train_df.reset_index(drop=True)\n",
        "val_df = val_df.reset_index(drop=True)\n",
        "test_df = test_df.reset_index(drop=True)\n",
        "\n",
        "# Substitua 'densidades' por uma coluna apropriada de etiquetas de classe\n",
        "train_dataset = TextoDataset(train_df['text'], train_df['labels'])\n",
        "validate_dataset = TextoDataset(val_df['text'], val_df['labels'])\n",
        "test_dataset = TextoDataset(test_df['text'], test_df['labels'])\n",
        "\n",
        "# Criar DataLoaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "validate_loader = DataLoader(validate_dataset, batch_size=BATCH_SIZE)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "JqN5Ljw1AOWi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class BertForClassification(nn.Module):\n",
        "    def __init__(self, bert_model, num_classes):\n",
        "        super(BertForClassification, self).__init__()\n",
        "        self.bert = bert_model\n",
        "        # Assuming 768-dimensional feature vectors from BERT, with num_classes outputs\n",
        "        self.classifier = nn.Linear(768, num_classes)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        return self.classifier(pooled_output)\n",
        "\n",
        "# Example usage\n",
        "bert_model = BertModel.from_pretrained('bert-base-uncased')  # Use a pre-trained BERT model\n",
        "model = BertForClassification(bert_model, num_classes=3)  # Specify the number of classes\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Loss function and optimizer for classification (using CrossEntropyLoss for multi-class)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-5)"
      ],
      "metadata": {
        "id": "mTgNjpyY9W1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, criterion, optimizer, train_loader, validate_loader, num_classes, epochs=3):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "\n",
        "        # Train loop with tqdm\n",
        "        train_loop = tqdm(train_loader, leave=True)\n",
        "        for input_ids, attention_mask, labels in train_loop:\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "\n",
        "            # For classification, outputs are likely to be logits, need to reshape for CrossEntropyLoss\n",
        "            outputs = outputs.view(-1, num_classes)  # num_classes is the number of classes in your classification task\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Update tqdm description with current loss\n",
        "        train_loop.set_description(f'Epoch {epoch+1}')\n",
        "        train_loop.set_postfix(loss=loss.item())\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        print('\\n Validating...')\n",
        "\n",
        "        # Validation loop with tqdm\n",
        "        validate_loop = tqdm(validate_loader, leave=True)\n",
        "        with torch.no_grad():\n",
        "            for input_ids, attention_mask, labels in validate_loop:\n",
        "                input_ids = input_ids.to(device)\n",
        "                attention_mask = attention_mask.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                outputs = model(input_ids, attention_mask)\n",
        "                outputs = outputs.view(-1, num_classes)  # Adjusted to match the number of classes\n",
        "\n",
        "                loss = criterion(outputs, labels)\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                validate_loop.set_postfix(val_loss=loss.item())\n",
        "\n",
        "        print(f\"Epoch {epoch+1}, Training Loss: {running_loss/len(train_loader)}, Validation Loss: {val_loss/len(validate_loader)}\")"
      ],
      "metadata": {
        "id": "O3CRjElj-GEW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(model, criterion, optimizer, train_loader, validate_loader, epochs=3, num_classes=EPOCHS)"
      ],
      "metadata": {
        "id": "WZ7BUEsjAVBe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the entire model\n",
        "torch.save(model, 'classification_model_balanced.pth')"
      ],
      "metadata": {
        "id": "1kz3iZ3vAVzK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, device):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_mask, labels in test_loader:\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            _, preds = torch.max(outputs, dim=1)\n",
        "\n",
        "            all_preds.extend(preds.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return np.array(all_preds), np.array(all_labels)\n",
        "\n",
        "# Get predictions and true labels\n",
        "preds, labels = evaluate_model(model, test_loader, device)\n",
        "\n",
        "# Total accuracy\n",
        "total_accuracy = accuracy_score(labels, preds)\n",
        "\n",
        "# Confusion matrix\n",
        "conf_matrix = confusion_matrix(labels, preds)\n",
        "per_class_accuracy = conf_matrix.diagonal() / conf_matrix.sum(axis=1)\n",
        "sensitivity = np.diag(conf_matrix) / np.sum(conf_matrix, axis=1)\n",
        "specificity = np.diag(conf_matrix) / np.sum(conf_matrix, axis=0)\n",
        "\n",
        "# Output the metrics\n",
        "print(\"Total Accuracy:\", total_accuracy)\n",
        "print(\"Per Class Accuracy:\", per_class_accuracy)\n",
        "print(\"Sensitivity (Recall) per Class:\", sensitivity)\n",
        "print(\"Specificity per Class:\", specificity)"
      ],
      "metadata": {
        "id": "o51JLEvgA1f9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GgEaiOUJA8Bv"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}